{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9bfe959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import time\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Models\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "# From your script\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# Project imports (adjust path if needed)\n",
    "import sys\n",
    "sys.path.append(\"c:/Projects/clasificador_mensajes\")\n",
    "from _data_layer import registry\n",
    "from _data_layer.registry import register_embedding_analysis, register_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "576745a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "# Notebook-friendly wrappers (keeps CLI working too)\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "from typing import Union, Sequence, Tuple\n",
    "\n",
    "def _in_notebook() -> bool:\n",
    "    try:\n",
    "        from IPython import get_ipython  # type: ignore\n",
    "        return get_ipython() is not None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def get_default_hdbscan_params() -> Dict[str, Any]:\n",
    "    # Mirrors your interactive_menu defaults\n",
    "    return {\n",
    "        \"pipeline_order\": \"umap-first\",\n",
    "        \"mode\": \"new\",\n",
    "        \"update_strategy\": \"retrain\",\n",
    "        # UMAP\n",
    "        \"n_neighbors\": 15,\n",
    "        \"n_components\": 2,\n",
    "        \"umap_metric\": \"cosine\",\n",
    "        # HDBSCAN\n",
    "        \"min_cluster_size\": 2,\n",
    "        \"min_samples\": 2,\n",
    "        \"cluster_selection_epsilon\": 0.0,\n",
    "        \"cluster_selection_method\": \"eom\",\n",
    "        \"hdb_metric\": \"euclidean\",\n",
    "    }\n",
    "\n",
    "def get_params(num_vectors: int,\n",
    "               overrides: Optional[Dict[str, Any]] = None,\n",
    "               interactive: bool = False) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    In notebooks: call with interactive=False and pass overrides to avoid input().\n",
    "    In CLI: interactive=True to show your prompt menu.\n",
    "    \"\"\"\n",
    "    if interactive and not _in_notebook():\n",
    "        return interactive_menu(num_vectors)\n",
    "    # non-interactive: start from defaults, apply overrides\n",
    "    p = get_default_hdbscan_params()\n",
    "    if overrides:\n",
    "        p.update({k: v for k, v in overrides.items() if v is not None})\n",
    "    return p\n",
    "\n",
    "def _make_ui_index(batches: List[Dict[str, Any]],\n",
    "                   gfilter: Optional[str],\n",
    "                   mfilter: Optional[str]) -> Tuple[List[int], List[Dict[str, Any]]]:\n",
    "    \"\"\"Return (shown_ui_indices, batches) after printing the table.\"\"\"\n",
    "    shown = list_batches(batches, gfilter, mfilter)\n",
    "    return shown, batches\n",
    "\n",
    "def _resolve_selection(selection: Union[str, Sequence[int]],\n",
    "                       shown: List[int]) -> List[int]:\n",
    "    if isinstance(selection, str):\n",
    "        if selection.strip().lower() == \"all\":\n",
    "            return list(range(len(shown)))\n",
    "        # support space-separated string in notebooks too\n",
    "        return _parse_space_indices(selection, len(shown))\n",
    "    # assume a list/tuple of UI indices\n",
    "    out = []\n",
    "    for i in selection:\n",
    "        if not (0 <= int(i) < len(shown)):\n",
    "            raise ValueError(f\"UI index out of range: {i}\")\n",
    "        if int(i) not in out:\n",
    "            out.append(int(i))\n",
    "    return out\n",
    "\n",
    "def select_and_run(selection: Union[str, Sequence[int]] = \"all\",\n",
    "                   gfilter: Optional[str] = None,\n",
    "                   mfilter: Optional[str] = None,\n",
    "                   params_overrides: Optional[Dict[str, Any]] = None,\n",
    "                   return_results: bool = True,\n",
    "                   timeit: bool = False):\n",
    "    \"\"\"\n",
    "    Notebook entrypoint.\n",
    "    Example:\n",
    "        shown, _ = _make_ui_index(fetch_batches(), gfilter=None, mfilter=None)\n",
    "        select_and_run(selection=[0,2], params_overrides={'n_neighbors':30, 'n_components':5})\n",
    "    \"\"\"\n",
    "    # 1) fetch & show\n",
    "    batches = fetch_batches()\n",
    "    shown, batches = _make_ui_index(batches, gfilter, mfilter)\n",
    "    if not shown:\n",
    "        print(\"(no batches match the current filters)\")\n",
    "        return None\n",
    "\n",
    "    sel_ui = _resolve_selection(selection, shown)\n",
    "\n",
    "    # 2) load & concat\n",
    "    try:\n",
    "        df, chosen_recs = _load_and_concat_selected(batches, shown, sel_ui)\n",
    "    except SystemExit as e:\n",
    "        print(str(e))\n",
    "        return None\n",
    "\n",
    "    gran_mix = sorted(set((_ensure_params_complete(r).get(\"granularity\") or \"?\") for r in chosen_recs))\n",
    "    models_mix = sorted(set((_ensure_params_complete(r).get(\"model\") or \"?\") for r in chosen_recs))\n",
    "    nrows = len(df)\n",
    "    print(f\"\\nâ€” Mix summary â€”  rows={nrows}  |  granularity={gran_mix}  |  model={models_mix}\")\n",
    "    print(\"  sources:\", [r.get(\"id\") for r in chosen_recs])\n",
    "\n",
    "    # 3) params (non-interactive by default in notebooks)\n",
    "    params = get_params(nrows, overrides=params_overrides, interactive=False)\n",
    "\n",
    "    # 4) matrix\n",
    "    X = np.vstack(df[\"embedding\"].to_numpy()).astype(\"float64\", copy=False)\n",
    "\n",
    "    # 5) pipeline\n",
    "    mode = params[\"mode\"]; update_strategy = params[\"update_strategy\"]\n",
    "    umap_path      = MODEL_DIR / \"umap_model.joblib\"\n",
    "    umap_proj_path = MODEL_DIR / \"umap_proj_model.joblib\"\n",
    "    hdb_path       = MODEL_DIR / \"hdbscan_model.joblib\"\n",
    "\n",
    "    # optional timing\n",
    "    import time\n",
    "    def _timed(fn, *a, **k):\n",
    "        t0 = time.perf_counter(); out = fn(*a, **k); dt = time.perf_counter() - t0\n",
    "        return out, dt\n",
    "\n",
    "    # 5.1) UMAP â†’ kD\n",
    "    k = max(2, int(params[\"n_components\"]))\n",
    "    if mode == \"update\" and umap_path.exists():\n",
    "        umap_model: umap.UMAP = joblib.load(umap_path)\n",
    "        if update_strategy == \"incremental\":\n",
    "            Z_k = umap_model.transform(X).astype(\"float64\", copy=False)\n",
    "            t_umap_k = 0.0\n",
    "        else:\n",
    "            X_full = np.vstack([umap_model._raw_data, X])  # type: ignore[attr-defined]\n",
    "            if timeit:\n",
    "                (umap_model, t_umap_k) = _timed(fit_umap, X_full, params[\"n_neighbors\"], k, params[\"umap_metric\"])\n",
    "            else:\n",
    "                umap_model = fit_umap(X_full, params[\"n_neighbors\"], k, params[\"umap_metric\"])\n",
    "                t_umap_k = 0.0\n",
    "            Z_k = umap_model.embedding_[-len(X):].astype(\"float64\", copy=False)\n",
    "    else:\n",
    "        if timeit:\n",
    "            (umap_model, t_umap_k) = _timed(fit_umap, X, params[\"n_neighbors\"], k, params[\"umap_metric\"])\n",
    "        else:\n",
    "            umap_model = fit_umap(X, params[\"n_neighbors\"], k, params[\"umap_metric\"])\n",
    "            t_umap_k = 0.0\n",
    "        Z_k = umap_model.embedding_.astype(\"float64\", copy=False)\n",
    "\n",
    "    if mode == \"new\" or update_strategy == \"retrain\":\n",
    "        joblib.dump(umap_model, umap_path)\n",
    "\n",
    "    # 5.2) HDBSCAN on kD\n",
    "    if mode == \"update\" and hdb_path.exists():\n",
    "        hdb_model: hdbscan.HDBSCAN = joblib.load(hdb_path)\n",
    "        if update_strategy == \"incremental\":\n",
    "            labels, strengths = hdbscan.approximate_predict(hdb_model, Z_k)\n",
    "            df[\"cluster\"] = labels; df[\"strength\"] = strengths\n",
    "            t_hdb = 0.0\n",
    "        else:\n",
    "            if timeit:\n",
    "                (hdb_model, t_hdb) = _timed(\n",
    "                    fit_hdbscan, Z_k,\n",
    "                    params[\"min_cluster_size\"], params[\"min_samples\"],\n",
    "                    params[\"cluster_selection_epsilon\"], params[\"cluster_selection_method\"],\n",
    "                    params[\"hdb_metric\"]\n",
    "                )\n",
    "            else:\n",
    "                hdb_model = fit_hdbscan(\n",
    "                    Z_k,\n",
    "                    params[\"min_cluster_size\"], params[\"min_samples\"],\n",
    "                    params[\"cluster_selection_epsilon\"], params[\"cluster_selection_method\"],\n",
    "                    params[\"hdb_metric\"]\n",
    "                )\n",
    "                t_hdb = 0.0\n",
    "            df[\"cluster\"]  = hdb_model.labels_\n",
    "            df[\"strength\"] = hdb_model.probabilities_\n",
    "    else:\n",
    "        if timeit:\n",
    "            (hdb_model, t_hdb) = _timed(\n",
    "                fit_hdbscan, Z_k,\n",
    "                params[\"min_cluster_size\"], params[\"min_samples\"],\n",
    "                params[\"cluster_selection_epsilon\"], params[\"cluster_selection_method\"],\n",
    "                params[\"hdb_metric\"]\n",
    "            )\n",
    "        else:\n",
    "            hdb_model = fit_hdbscan(\n",
    "                Z_k,\n",
    "                params[\"min_cluster_size\"], params[\"min_samples\"],\n",
    "                params[\"cluster_selection_epsilon\"], params[\"cluster_selection_method\"],\n",
    "                params[\"hdb_metric\"]\n",
    "            )\n",
    "            t_hdb = 0.0\n",
    "        df[\"cluster\"]  = hdb_model.labels_\n",
    "        df[\"strength\"] = hdb_model.probabilities_\n",
    "\n",
    "    if mode == \"new\" or update_strategy == \"retrain\":\n",
    "        joblib.dump(hdb_model, hdb_path)\n",
    "\n",
    "    # 5.3) UMAP â†’ 2D projection (for plotting only)\n",
    "    if mode == \"update\" and umap_proj_path.exists() and update_strategy == \"incremental\":\n",
    "        umap_proj: umap.UMAP = joblib.load(umap_proj_path)\n",
    "        embedding_2d = umap_proj.transform(Z_k).astype(\"float64\", copy=False)\n",
    "        t_umap_2d = 0.0\n",
    "    else:\n",
    "        if timeit:\n",
    "            (umap_proj, t_umap_2d) = _timed(\n",
    "                fit_umap, Z_k,\n",
    "                n_neighbors=max(30, params[\"n_neighbors\"] // 2),\n",
    "                n_components=2, metric=\"euclidean\"\n",
    "            )\n",
    "        else:\n",
    "            umap_proj = fit_umap(Z_k, n_neighbors=max(30, params[\"n_neighbors\"] // 2), n_components=2, metric=\"euclidean\")\n",
    "            t_umap_2d = 0.0\n",
    "        embedding_2d = umap_proj.embedding_.astype(\"float64\", copy=False)\n",
    "\n",
    "    if mode == \"new\" or update_strategy == \"retrain\":\n",
    "        joblib.dump(umap_proj, umap_proj_path)\n",
    "\n",
    "    # 6) persist experiment & register\n",
    "    df = df.copy()\n",
    "    if \"embedding\" in df.columns:\n",
    "        del df[\"embedding\"]\n",
    "    df[\"umap_x\"], df[\"umap_y\"] = embedding_2d[:, 0], embedding_2d[:, 1]\n",
    "\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    out_path = EXPER_DIR / f\"exp_mix_{ts}.joblib\"\n",
    "    joblib.dump({\"df\": df,\n",
    "                 \"source_batch_ids\": [r.get(\"id\") for r in chosen_recs],\n",
    "                 \"params\": params},\n",
    "                out_path, compress=3)\n",
    "    print(f\"\\nâœ… Results saved â†’ {out_path.name}\")\n",
    "\n",
    "    try:\n",
    "        umap_model_id = register_model(\n",
    "            data_ref=str(umap_path),\n",
    "            parameters={\n",
    "                \"n_neighbors\": params[\"n_neighbors\"],\n",
    "                \"n_components\": params[\"n_components\"],\n",
    "                \"metric\": params[\"umap_metric\"],\n",
    "            },\n",
    "            parents=[r.get(\"id\") for r in chosen_recs],\n",
    "            model_type=\"umap\"\n",
    "        )\n",
    "        hdbscan_model_id = register_model(\n",
    "            data_ref=str(hdb_path),\n",
    "            parameters={\n",
    "                \"min_cluster_size\":          params[\"min_cluster_size\"],\n",
    "                \"min_samples\":               params[\"min_samples\"],\n",
    "                \"cluster_selection_epsilon\": params[\"cluster_selection_epsilon\"],\n",
    "                \"cluster_selection_method\":  params[\"cluster_selection_method\"],\n",
    "                \"metric\":                    params[\"hdb_metric\"],\n",
    "            },\n",
    "            parents=[r.get(\"id\") for r in chosen_recs],\n",
    "            model_type=\"hdbscan\"\n",
    "        )\n",
    "        register_embedding_analysis(\n",
    "            data_ref=str(out_path),\n",
    "            parameters={\n",
    "                \"pipeline_order\": params[\"pipeline_order\"],\n",
    "                \"mode\": mode,\n",
    "                \"update_strategy\": update_strategy,\n",
    "                \"umap_model\": umap_model_id,\n",
    "                \"hdbscan_model\": hdbscan_model_id,\n",
    "                \"granularity_mix\": sorted(set((_ensure_params_complete(r).get(\"granularity\") or \"?\") for r in chosen_recs)),\n",
    "                \"source_models\": sorted(set((_ensure_params_complete(r).get(\"model\") or \"?\") for r in chosen_recs)),\n",
    "                \"source_n_vectors_total\": int(len(df)),\n",
    "                \"source_batches\": [r.get(\"batches\") for r in chosen_recs],\n",
    "                \"source_batch_ids\": [r.get(\"id\") for r in chosen_recs],\n",
    "            },\n",
    "            parents=[r.get(\"id\") for r in chosen_recs]\n",
    "        )\n",
    "        print(\"ðŸ§­ Registered analysis with lineage to the selected vector batches.\")\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ Registry registration failed (continuing):\", e)\n",
    "\n",
    "    if timeit and (t_umap_k or t_hdb or t_umap_2d):\n",
    "        total = (t_umap_k + t_hdb + t_umap_2d)\n",
    "        print(f\"\\nâ± Timing: UMAP_kD={t_umap_k:.2f}s  |  HDBSCAN={t_hdb:.2f}s  |  UMAP_2D={t_umap_2d:.2f}s  |  total={total:.2f}s\")\n",
    "\n",
    "    return (df, str(out_path), str(umap_path), str(hdb_path), str(umap_proj_path)) if return_results else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ae1d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keep your CLI for terminals\n",
    "def main():\n",
    "    gfilter: Optional[str] = None  # \"message\" | \"conversation\" | None\n",
    "    mfilter: Optional[str] = None  # substring on model\n",
    "\n",
    "    # If running inside a notebook, bail out early to avoid input() loops\n",
    "    if _in_notebook():\n",
    "        print(\"Notebook detected â€” use select_and_run(...) instead of main().\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        batches = fetch_batches()\n",
    "        shown = list_batches(batches, gfilter, mfilter)\n",
    "\n",
    "        print(\"\\nOptions:\")\n",
    "        print(\"  [space-separated indices] Analyze those batches (e.g., '1 3 7' or 'all')\")\n",
    "        print(\"  g) Set granularity filter (message / conversation / all)\")\n",
    "        print(\"  m) Set model filter (substring, empty to clear)\")\n",
    "        print(\"  r) Refresh list\")\n",
    "        print(\"  q) Quit\")\n",
    "\n",
    "        choice = input(\"Select: \").strip().lower()\n",
    "        if choice == \"q\":\n",
    "            return\n",
    "        elif choice == \"r\":\n",
    "            continue\n",
    "        elif choice == \"g\":\n",
    "            val = input(\"Granularity (message / conversation / all): \").strip().lower()\n",
    "            gfilter = val if val in (\"message\", \"conversation\") else None\n",
    "            continue\n",
    "        elif choice == \"m\":\n",
    "            val = input(\"Model filter substring (blank to clear): \").strip()\n",
    "            mfilter = val or None\n",
    "            continue\n",
    "        else:\n",
    "            if choice == \"all\":\n",
    "                sel_ui = list(range(len(shown)))\n",
    "            else:\n",
    "                try:\n",
    "                    sel_ui = _parse_space_indices(choice, len(shown))\n",
    "                except ValueError as e:\n",
    "                    print(\"Selection error:\", e)\n",
    "                    continue\n",
    "\n",
    "            try:\n",
    "                df, chosen_recs = _load_and_concat_selected(batches, shown, sel_ui)\n",
    "            except SystemExit as e:\n",
    "                print(str(e)); input(\"\\n(Press Enter to go back)\"); continue\n",
    "\n",
    "            gran_mix = sorted(set((_ensure_params_complete(r).get(\"granularity\") or \"?\") for r in chosen_recs))\n",
    "            models_mix = sorted(set((_ensure_params_complete(r).get(\"model\") or \"?\") for r in chosen_recs))\n",
    "            nrows = len(df)\n",
    "            print(f\"\\nâ€” Mix summary â€”  rows={nrows}  |  granularity={gran_mix}  |  model={models_mix}\")\n",
    "            print(\"  sources:\", [r.get(\"id\") for r in chosen_recs])\n",
    "\n",
    "            params = interactive_menu(nrows)\n",
    "\n",
    "            X = np.vstack(df[\"embedding\"].to_numpy()).astype(\"float64\", copy=False)\n",
    "\n",
    "            # (the rest of your original run bodyâ€¦ you already have it below)\n",
    "            # Tip: you can reuse select_and_run here if you want, but I left your original CLI intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "546ca558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define helpers (copied from your script) ---\n",
    "def fit_umap(X: np.ndarray, n_neighbors: int, n_components: int, metric: str) -> umap.UMAP:\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        n_components=n_components,\n",
    "        metric=metric,\n",
    "        random_state=42\n",
    "    ).fit(X)\n",
    "\n",
    "def fit_hdbscan(X: np.ndarray,\n",
    "                min_cluster_size: int,\n",
    "                min_samples: int,\n",
    "                cluster_selection_epsilon: float,\n",
    "                cluster_selection_method: str,\n",
    "                metric: str) -> hdbscan.HDBSCAN:\n",
    "    return hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        cluster_selection_epsilon=cluster_selection_epsilon,\n",
    "        cluster_selection_method=cluster_selection_method,\n",
    "        metric=metric,\n",
    "        algorithm=\"generic\",\n",
    "        prediction_data=True,\n",
    "    ).fit(X)\n",
    "\n",
    "# --- Timing helper ---\n",
    "def timeit(func, *args, **kwargs):\n",
    "    import time\n",
    "    t0 = time.perf_counter()\n",
    "    result = func(*args, **kwargs)\n",
    "    t1 = time.perf_counter()\n",
    "    print(f\"{func.__name__} took {t1 - t0:.3f} seconds\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "022ccab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timeit(func, *args, **kwargs):\n",
    "    t0 = time.perf_counter()\n",
    "    result = func(*args, **kwargs)\n",
    "    t1 = time.perf_counter()\n",
    "    print(f\"{func.__name__} took {t1 - t0:.3f} seconds\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fdfd28a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path/to/your/batch.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 1) Load a batch (adapt path to your data)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath/to/your/batch.joblib\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m obj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m obj\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 2) Build X matrix\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/your/batch.joblib'"
     ]
    }
   ],
   "source": [
    "# 1) Load a batch (adapt path to your data)\n",
    "import joblib\n",
    "obj = joblib.load(\"path/to/your/batch.joblib\")\n",
    "df = obj[\"df\"] if isinstance(obj, dict) else obj\n",
    "\n",
    "# 2) Build X matrix\n",
    "import numpy as np\n",
    "X = np.vstack(df[\"embedding\"].to_numpy()).astype(\"float64\", copy=False)\n",
    "\n",
    "# 3) Time UMAP and HDBSCAN\n",
    "umap_model = timeit(fit_umap, X, n_neighbors=15, n_components=2, metric=\"cosine\")\n",
    "Z = umap_model.embedding_\n",
    "\n",
    "hdb_model = timeit(\n",
    "    fit_hdbscan,\n",
    "    Z,\n",
    "    min_cluster_size=2,\n",
    "    min_samples=2,\n",
    "    cluster_selection_epsilon=0.0,\n",
    "    cluster_selection_method=\"eom\",\n",
    "    metric=\"euclidean\"\n",
    ")\n",
    "\n",
    "labels = hdb_model.labels_\n",
    "strengths = hdb_model.probabilities_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b592cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
