{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f735cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q joblib pandas numpy pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f1acd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF shape: (27463, 14)\n",
      "\n",
      "ALL COLUMNS:\n",
      "['conversation_id', 'batch_id', 'message_id', 'sender', 'timestamp', 'text', 'message_hash', 'granularity', 'source_batch_id', 'source_model', 'cluster', 'strength', 'umap_x', 'umap_y']\n",
      "\n",
      "DTYPES:\n",
      "conversation_id     object\n",
      "batch_id            object\n",
      "message_id          object\n",
      "sender              object\n",
      "timestamp           object\n",
      "text                object\n",
      "message_hash        object\n",
      "granularity         object\n",
      "source_batch_id     object\n",
      "source_model        object\n",
      "cluster              int64\n",
      "strength           float64\n",
      "umap_x             float64\n",
      "umap_y             float64\n",
      "\n",
      "HEAD(3):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "conversation_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "batch_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "message_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sender",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "timestamp",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "message_hash",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "granularity",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source_batch_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source_model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cluster",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "strength",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "umap_x",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "umap_y",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "2abe3ba6-2973-454c-8bf1-ed951d21c0b3",
       "rows": [
        [
         "0",
         "conv_42a3e6cf",
         "jsonl_import",
         "msg_71c55fc69ed0",
         "user",
         "2025-09-25T21:04:05Z",
         "lets do a debugger of the parser--- in ipynb notebook",
         "c01682962297ed7d0a76e1b2b993a00f2a18f4b9d7c8b17028a84a45ac961191",
         "message",
         "vectors_batch_f953107ed8",
         "text-embedding-3-small",
         "1089",
         "1.0",
         "-6.272158622741699",
         "-0.6726643443107605"
        ],
        [
         "1",
         "conv_42a3e6cf",
         "jsonl_import",
         "msg_b765f8a4a016",
         "user",
         "2025-09-25T21:08:29Z",
         "lets do a debugger of the parser--- give me cells i will copy them to a ipynb",
         "3220c26f2ab21efe8a9023edc93cb2f7d083b9c80595ebf8757d8283ab7bf695",
         "message",
         "vectors_batch_f953107ed8",
         "text-embedding-3-small",
         "1089",
         "1.0",
         "-6.264522075653076",
         "-0.6642675399780273"
        ],
        [
         "2",
         "conv_42a3e6cf",
         "jsonl_import",
         "msg_dab4ac7c3b28",
         "tool",
         "2025-09-25T21:04:06Z",
         "Make sure to include fileciteturn0file0 in your response to cite this file. \n\n# src/proc/parse_openai.py\nimport os\nimport io\nimport re\nimport json\nimport hashlib\nimport zipfile\nfrom datetime import datetime, timezone\nfrom typing import Dict, Any, List, Tuple, Optional\n\nimport pandas as pd\n\n# Use your existing data layer\nfrom _data_layer import api\n\n# ------------------------------\n# Config via environment\n# ------------------------------\n# Where to store extracted media (outside Git; dedup by sha256)\nMEDIA_ROOT = os.getenv(\"MEDIA_ROOT\", r\"D:\\data_hub\\media\\openai_chat\")\n# Dataset id used for curated stage\nDEFAULT_DATASET_ID = os.getenv(\"DATASET_ID\", \"openai_chat\")\n\n\n# ------------------------------\n# Utilities\n# ------------------------------\ndef _sha256_bytes(b: bytes) -> str:\n    return hashlib.sha256(b).hexdigest()\n\n\ndef _sha256_text(s: str) -> str:\n    return hashlib.sha256((s or \"\").strip().encode(\"utf-8\")).hexdigest()\n\n\ndef _now_iso() -> str:\n    return datetime.now(timezone.utc).isoformat()\n\n\ndef _to_utc_iso(ts) -> str:\n    \"\"\"Best-effort normalization to UTC ISO 8601.\"\"\"\n    try:\n        if isinstance(ts, (int, float)):  # epoch seconds\n            return datetime.fromtimestamp(ts, tz=timezone.utc).isoformat()\n        if isinstance(ts, str):\n            # Allow both Z and offset strings\n            s = ts.replace(\"Z\", \"+00:00\")\n            return datetime.fromisoformat(s).astimezone(timezone.utc).isoformat()\n    except Exception:\n        pass\n    return _now_iso()\n\n\ndef _safe_mkdir(p: str) -> None:\n    os.makedirs(p, exist_ok=True)\n\n\ndef _infer_ext_from_mime_or_name(name_or_mime: str) -> str:\n    # very light heuristic; extend as needed\n    s = (name_or_mime or \"\").lower()\n    for ext in (\".png\", \".jpg\", \".jpeg\", \".gif\", \".webp\", \".bmp\", \".svg\", \".pdf\", \".txt\"):\n        if s.endswith(ext) or ext in s:\n            return ext\n    if \"image/png\" in s: return \".png\"\n    if \"image/jpeg\" in s: return \".jpg\"\n    if \"image/gif\" in s: return \".gif\"\n    if \"image/webp\" in s: return \".webp\"\n    if \"application/pdf\" in s: return \".pdf\"\n    return \".bin\"\n\n\ndef _kind_from_ext(ext: str) -> str:\n    ext = (ext or \"\").lower()\n    if ext in [\".png\", \".jpg\", \".jpeg\", \".gif\", \".webp\", \".bmp\", \".svg\"]: return \"image\"\n    if ext in [\".pdf\"]: return \"file\"\n    return \"file\"\n\n\ndef _uuid5_like(ns: str, name: str) -> str:\n    # Deterministic id without importing uuid (keeps deps minimal)\n    return hashlib.sha1((ns + \":\" + name).encode(\"utf-8\")).hexdigest()  # stable enough for ids\n\n\n# ------------------------------\n# JSON candidates inside ZIP\n# ------------------------------\nCANDIDATE_JSON_PATTERNS = [\n    re.compile(r\"^conversations/.*\\.json$\", re.I),\n    re.compile(r\"^chats?/.*\\.json$\", re.I),\n    re.compile(r\"messages\\.json$\", re.I),\n    re.compile(r\"conversations\\.json$\", re.I),\n    re.compile(r\"chat_export\\.json$\", re.I),\n]\n\n\ndef _is_json_candidate(name: str) -> bool:\n    n = name.replace(\"\\\\\", \"/\")\n    return any(p.search(n) for p in CANDIDATE_JSON_PATTERNS)\n\n\n# ------------------------------\n# Media extraction (embedded in ZIP only)\n# ------------------------------\ndef _extract_media_from_zip(z: zipfile.ZipFile, internal_path: str) -> Optional[Tuple[str, int, str]]:\n    \"\"\"\n    If the ZIP contains media under internal_path, extract bytes, write to MEDIA_ROOT dedup store, return (stored_path, size_bytes, sha256).\n    If not present, returns None (e.g., external URL-only reference).\n    \"\"\"\n    try:\n        with z.open(internal_path) as fh:\n            b = fh.read()\n        sha = _sha256_bytes(b)\n        ext = _infer_ext_from_mime_or_name(internal_path)\n        obj_dir = os.path.join(MEDIA_ROOT, \"objects\")\n        _safe_mkdir(obj_dir)\n        stored_path = os.path.join(obj_dir, f\"{sha}{ext}\")\n        if not os.path.exists(stored_path):\n            with open(stored_path, \"wb\") as out:\n                out.write(b)\n        return stored_path, len(b), sha\n    except KeyError:\n        # Not embedded in ZIP\n        return None\n    except Exception:\n        return None\n\n\n# ------------------------------\n# Parsers\n# ------------------------------\ndef _parse_new_format_messages(data: Dict[str, Any], json_path: str) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n    \"\"\"\n    Newer shape:\n    {\"messages\":[\n      {\"id\":\"...\",\"conversation_id\":\"...\",\"created_at\":1695000,\n       \"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":\"...\"}, {\"type\":\"image_url\",\"url\":\"...\"}]}\n    ]}\n    Returns (curated_rows, media_rows)\n    \"\"\"\n    curated_rows, media_rows = [], []\n    msgs = data.get(\"messages\")\n    if not isinstance(msgs, list):\n        return curated_rows, media_rows\n\n    unit_default = os.path.splitext(os.path.basename(json_path))[0]\n\n    for i, m in enumerate(msgs):\n        unit_uid = m.get(\"conversation_id\") or unit_default\n        segment_id = m.get(\"id\") or f\"seg_{i:06d}\"\n        created_at = _to_utc_iso(m.get(\"created_at\") or m.get(\"create_time\") or m.get(\"timestamp\"))\n        role = m.get(\"role\") or (m.get(\"author\") or {}).get(\"role\")\n\n        text_parts = []\n        content = m.get(\"content\")\n        if isinstance(content, list):\n            for part in content:\n                if not isinstance(part, dict): \n                    continue\n                t = (part.get(\"type\") or \"\").lower()\n                if t == \"text\":\n                    text = part.get(\"text\") or part.get(\"value\") or \"\"\n                    if text:\n                        text_parts.append(text)\n                elif t in (\"image_url\", \"file\", \"attachment\"):\n                    # Track media reference; if inside ZIP, we'll try to extract with same path\n                    url_or_path = part.get(\"url\") or part.get(\"path\") or \"\"\n                    ext = _infer_ext_from_mime_or_name(url_or_path)\n                    kind = _kind_from_ext(ext)\n                    media_id = _uuid5_like(unit_uid, f\"{segment_id}:{url_or_path or t}\")\n                    media_rows.append({\n                        \"media_id\": media_id,\n                        \"unit_uid\": unit_uid,\n                        \"segment_id\": segment_id,\n                        \"kind\": kind,\n                        \"filename\": os.path.basename(url_or_path) or f\"{media_id}{ext}\",\n                        \"ext\": ext,\n                        \"size_bytes\": None,\n                        \"sha256\": None,\n                        \"zip_internal_path\": url_or_path if url_or_path else None,\n                        \"stored_path\": None,  # may be filled if embedded\n                        \"source_ref\": json_path,\n                        \"created_at\": created_at,\n                    })\n        elif isinstance(content, str):\n            text_parts.append(content)\n\n        text_concat = \"\\n\".join([t for t in text_parts if t])\n        curated_rows.append({\n            \"unit_uid\": unit_uid,\n            \"segment_id\": segment_id,\n            \"created_at\": created_at,\n            \"text\": text_concat,\n            \"source_kind\": \"openai_chat\",\n            \"source_ref\": json_path,\n            \"content_hash\": _sha256_text(text_concat),\n            \"role\": role or None,\n        })\n\n    return curated_rows, media_rows\n\n\ndef _parse_legacy_mapping(data: Dict[str, Any], json_path: str) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n    \"\"\"\n    Legacy shape with a 'mapping' graph.\n    Returns (curated_rows, media_rows)\n    \"\"\"\n    curated_rows, media_rows = [], []\n    mapping = data.get(\"mapping\")\n    if not isinstance(mapping, dict):\n        return curated_rows, media_rows\n\n    unit_default = os.path.splitext(os.path.basename(json_path))[0]\n    nodes = list(mapping.values())\n\n    # Preserve order by 'create_time' if present, else by index\n    def _node_time(n):\n        msg = n.get(\"message\", {})\n        return msg.get(\"create_time\") or msg.get(\"created_at\") or 0\n\n    nodes.sort(key=_node_time)\n\n    for idx, node in enumerate(nodes):\n        msg = node.get(\"message\") or {}\n        # Some nodes are structural, skip if no content\n        cont = msg.get(\"content\")\n        if not cont:\n            continue\n\n        unit_uid = msg.get(\"conversation_id\") or unit_default\n        segment_id = msg.get(\"id\") or f\"seg_{idx:06d}\"\n        created_at = _to_utc_iso(msg.get(\"create_time\") or msg.get(\"created_at\") or 0)\n        role = (msg.get(\"author\") or {}).get(\"role\")\n\n        text_parts = []\n        # legacy often stores content.parts: [\"text\", \"...\"]\n        parts = cont.get(\"parts\") if isinstance(cont, dict) else None\n        if isinstance(parts, list):\n            for p in parts:\n                if isinstance(p, str):\n                    text_parts.append(p)\n                elif isinstance(p, dict):\n                    # very rare for legacy, but just in case\n                    if (p.get(\"type\") or \"\").lower() == \"text\":\n                        text_parts.append(p.get(\"text\") or \"\")\n        else:\n            # Sometimes 'content' is directly a string\n            if isinstance(cont, str):\n                text_parts.append(cont)\n\n        text_concat = \"\\n\".join([t for t in text_parts if t])\n        curated_rows.append({\n            \"unit_uid\": unit_uid,\n            \"segment_id\": segment_id,\n            \"created_at\": created_at,\n            \"text\": text_concat,\n            \"source_kind\": \"openai_chat\",\n            \"source_ref\": json_path,\n            \"content_hash\": _sha256_text(text_concat),\n            \"role\": role or None,\n        })\n\n    return curated_rows, media_rows\n\n\n# ------------------------------\n# Main entry\n# ------------------------------\ndef parse_export_to_curated_v2(zip_path: str, dataset_id: str = DEFAULT_DATASET_ID) -> Dict[str, Any]:\n    \"\"\"\n    Parse a ChatGPT export ZIP:\n      - Find JSON candidates\n      - Normalize to curated v2 rows (tiny schema)\n      - Extract embedded media to MEDIA_ROOT (dedup by sha256)\n      - Save:\n          * curated:<dataset_id>\n          * media_manifest:<dataset_id>   (if any media found)\n    Returns the curated artifact registry record (dict).\n    \"\"\"\n    curated_rows: List[Dict[str, Any]] = []\n    media_rows: List[Dict[str, Any]] = []\n\n    with zipfile.ZipFile(zip_path) as z:\n        names = [n for n in z.namelist() if _is_json_candidate(n)]\n        for name in names:\n            try:\n                with z.open(name) as f:\n                    raw = f.read()\n                data = json.loads(raw.decode(\"utf-8\", errors=\"ignore\"))\n            except Exception:\n                continue\n\n            # Try new format first\n            rows_new, media_new = _parse_new_format_messages(data, json_path=name)\n            if rows_new:\n                curated_rows.extend(rows_new)\n                media_rows.extend(media_new)\n                continue\n\n            # Fallback to legacy mapping\n            rows_old, media_old = _parse_legacy_mapping(data, json_path=name)\n            if rows_old:\n                curated_rows.extend(rows_old)\n                media_rows.extend(media_old)\n\n        # Attempt to materialize media that is actually embedded in the ZIP\n        realized_media_rows: List[Dict[str, Any]] = []\n        for mr in media_rows:\n            ipath = mr.get(\"zip_internal_path\")\n            if ipath:\n                extracted = _extract_media_from_zip(z, ipath)\n                if extracted:\n                    stored_path, size_bytes, sha = extracted\n                    mr = dict(mr)\n                    mr[\"stored_path\"] = stored_path\n                    mr[\"size_bytes\"] = size_bytes\n                    mr[\"sha256\"] = sha\n            realized_media_rows.append(mr)\n\n    # If no rows (unexpected), emit a safe demo row to keep pipeline shape intact\n    if not curated_rows:\n        curated_rows = [{\n            \"unit_uid\": \"demo_unit\",\n            \"segment_id\": \"seg_000001\",\n            \"created_at\": _now_iso(),\n            \"text\": \"Demo row (no messages found in export).\",\n            \"source_kind\": \"openai_chat\",\n            \"source_ref\": os.path.basename(zip_path),\n            \"content_hash\": _sha256_text(\"Demo row (no messages found in export).\"),\n            \"role\": None,\n        }]\n\n    curated_df = pd.DataFrame(curated_rows)\n    # Keep only minimal curated v2 columns; extra columns (role) are okay but optional\n    curated_df = curated_df[[\n        \"unit_uid\", \"segment_id\", \"created_at\", \"text\",\n        \"source_kind\", \"source_ref\", \"content_hash\", \"role\"\n    ]].copy()\n\n    curated_art = api.save_artifact(\n        curated_df,\n        stage=f\"curated:{dataset_id}\",\n        parameters={\n            \"dataset_id\": dataset_id,\n            \"version\": datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n            \"schema\": {\n                \"unit_uid\": \"str\",\n                \"segment_id\": \"str\",\n                \"created_at\": \"datetime_iso\",\n                \"text\": \"str\",\n                \"source_kind\": \"str\",\n                \"source_ref\": \"str\",\n                \"content_hash\": \"str\",\n                \"role\": \"str?\",\n            },\n            \"notes\": \"Curated v2 (tiny schema). Text-only; media tracked separately in media_manifest.\"\n        }\n    )\n\n    # Save media manifest if any\n    if realized_media_rows:\n        media_df = pd.DataFrame(realized_media_rows)\n        # Ensure consistent columns\n        for col in [\"media_id\",\"unit_uid\",\"segment_id\",\"kind\",\"filename\",\"ext\",\"size_bytes\",\"sha256\",\"zip_internal_path\",\"stored_path\",\"source_ref\",\"created_at\"]:\n            if col not in media_df.columns:\n                media_df[col] = None\n\n        api.save_artifact(\n            media_df,\n            stage=f\"media_manifest:{dataset_id}\",\n            parameters={\"dataset_id\": dataset_id},\n            parents=[curated_art[\"id\"]],\n        )\n\n    return curated_art",
         "d1c6795285e96af12fa6cd5eb57a24fe4f228f8d2ac65d9bd80316a484bf0434",
         "message",
         "vectors_batch_f953107ed8",
         "text-embedding-3-small",
         "2456",
         "1.0",
         "-3.9193711280822754",
         "16.44688606262207"
        ]
       ],
       "shape": {
        "columns": 14,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>message_id</th>\n",
       "      <th>sender</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>message_hash</th>\n",
       "      <th>granularity</th>\n",
       "      <th>source_batch_id</th>\n",
       "      <th>source_model</th>\n",
       "      <th>cluster</th>\n",
       "      <th>strength</th>\n",
       "      <th>umap_x</th>\n",
       "      <th>umap_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>conv_42a3e6cf</td>\n",
       "      <td>jsonl_import</td>\n",
       "      <td>msg_71c55fc69ed0</td>\n",
       "      <td>user</td>\n",
       "      <td>2025-09-25T21:04:05Z</td>\n",
       "      <td>lets do a debugger of the parser--- in ipynb notebook</td>\n",
       "      <td>c01682962297ed7d0a76e1b2b993a00f2a18f4b9d7c8b17028a84a45ac961191</td>\n",
       "      <td>message</td>\n",
       "      <td>vectors_batch_f953107ed8</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>1089</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.272159</td>\n",
       "      <td>-0.672664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>conv_42a3e6cf</td>\n",
       "      <td>jsonl_import</td>\n",
       "      <td>msg_b765f8a4a016</td>\n",
       "      <td>user</td>\n",
       "      <td>2025-09-25T21:08:29Z</td>\n",
       "      <td>lets do a debugger of the parser--- give me cells i will copy them to a ipynb</td>\n",
       "      <td>3220c26f2ab21efe8a9023edc93cb2f7d083b9c80595ebf8757d8283ab7bf695</td>\n",
       "      <td>message</td>\n",
       "      <td>vectors_batch_f953107ed8</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>1089</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.264522</td>\n",
       "      <td>-0.664268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conv_42a3e6cf</td>\n",
       "      <td>jsonl_import</td>\n",
       "      <td>msg_dab4ac7c3b28</td>\n",
       "      <td>tool</td>\n",
       "      <td>2025-09-25T21:04:06Z</td>\n",
       "      <td>Make sure to include fileciteturn0file0 in your response to cite this file. \\n\\n# src/proc/parse_openai.py\\nimport os\\nimport io\\nimport re\\nimport json\\nimport hashlib\\nimport zipfile\\nfrom datetime import datetime, timezone\\nfrom typing import Dict, Any, List, Tuple, Optional\\n\\nimport pandas as pd\\n\\n# Use your existing data layer\\nfrom _data_layer import api\\n\\n# ------------------------------\\n# Config via environment\\n# ------------------------------\\n# Where to store extracted media (outside Git; dedup by sha256)\\nMEDIA_ROOT = os.getenv(\"MEDIA_ROOT\", r\"D:\\data_hub\\media\\openai_chat\")\\n# Dataset id used for curated stage\\nDEFAULT_DATASET_ID = os.getenv(\"DATASET_ID\", \"openai_chat\")\\n\\n\\n# ------------------------------\\n# Utilities\\n# ------------------------------\\ndef _sha256_bytes(b: bytes) -&gt; str:\\n    return hashlib.sha256(b).hexdigest()\\n\\n\\ndef _sha256_text(s: str) -&gt; str:\\n    return hashlib.sha256((s or \"\").strip().encode(\"utf-8\")).hexdigest()\\n\\n\\ndef _now_iso() -&gt; str:\\n    return datetime.now(timezone.utc).isoformat()\\n\\n\\ndef _to_utc_iso(ts) -&gt; str:\\n    \"\"\"Best-effort normalization to UTC ISO 8601.\"\"\"\\n    try:\\n        if isinstance(ts, (int, float)):  # epoch seconds\\n            return datetime.fromtimestamp(ts, tz=timezone.utc).isoformat()\\n        if isinstance(ts, str):\\n            # Allow both Z and offset strings\\n            s = ts.replace(\"Z\", \"+00:00\")\\n            return datetime.fromisoformat(s).astimezone(timezone.utc).isoformat()\\n    except Exception:\\n        pass\\n    return _now_iso()\\n\\n\\ndef _safe_mkdir(p: str) -&gt; None:\\n    os.makedirs(p, exist_ok=True)\\n\\n\\ndef _infer_ext_from_mime_or_name(name_or_mime: str) -&gt; str:\\n    # very light heuristic; extend as needed\\n    s = (name_or_mime or \"\").lower()\\n    for ext in (\".png\", \".jpg\", \".jpeg\", \".gif\", \".webp\", \".bmp\", \".svg\", \".pdf\", \".txt\"):\\n        if s.endswith(ext) or ext in s:\\n            return ext\\n    if \"image/png\" in s: return \".png\"\\n    if \"image/jpeg\" in s: return \".jpg\"\\n    if \"image/gif\" in s: return \".gif\"\\n    if \"image/webp\" in s: return \".webp\"\\n    if \"application/pdf\" in s: return \".pdf\"\\n    return \".bin\"\\n\\n\\ndef _kind_from_ext(ext: str) -&gt; str:\\n    ext = (ext or \"\").lower()\\n    if ext in [\".png\", \".jpg\", \".jpeg\", \".gif\", \".webp\", \".bmp\", \".svg\"]: return \"image\"\\n    if ext in [\".pdf\"]: return \"file\"\\n    return \"file\"\\n\\n\\ndef _uuid5_like(ns: str, name: str) -&gt; str:\\n    # Deterministic id without importing uuid (keeps deps minimal)\\n    return hashlib.sha1((ns + \":\" + name).encode(\"utf-8\")).hexdigest()  # stable enough for ids\\n\\n\\n# ------------------------------\\n# JSON candidates inside ZIP\\n# ------------------------------\\nCANDIDATE_JSON_PATTERNS = [\\n    re.compile(r\"^conversations/.*\\.json$\", re.I),\\n    re.compile(r\"^chats?/.*\\.json$\", re.I),\\n    re.compile(r\"messages\\.json$\", re.I),\\n    re.compile(r\"conversations\\.json$\", re.I),\\n    re.compile(r\"chat_export\\.json$\", re.I),\\n]\\n\\n\\ndef _is_json_candidate(name: str) -&gt; bool:\\n    n = name.replace(\"\\\\\", \"/\")\\n    return any(p.search(n) for p in CANDIDATE_JSON_PATTERNS)\\n\\n\\n# ------------------------------\\n# Media extraction (embedded in ZIP only)\\n# ------------------------------\\ndef _extract_media_from_zip(z: zipfile.ZipFile, internal_path: str) -&gt; Optional[Tuple[str, int, str]]:\\n    \"\"\"\\n    If the ZIP contains media under internal_path, extract bytes, write to MEDIA_ROOT dedup store, return (stored_path, size_bytes, sha256).\\n    If not present, returns None (e.g., external URL-only reference).\\n    \"\"\"\\n    try:\\n        with z.open(internal_path) as fh:\\n            b = fh.read()\\n        sha = _sha256_bytes(b)\\n        ext = _infer_ext_from_mime_or_name(internal_path)\\n        obj_dir = os.path.join(MEDIA_ROOT, \"objects\")\\n        _safe_mkdir(obj_dir)\\n        stored_path = os.path.join(obj_dir, f\"{sha}{ext}\")\\n        if not os.path.exists(stored_path):\\n            with open(stored_path, \"wb\") as out:\\n                out.write(b)\\n        return stored_path, len(b), sha\\n    except KeyError:\\n        # Not embedded in ZIP\\n        return None\\n    except Exception:\\n        return None\\n\\n\\n# ------------------------------\\n# Parsers\\n# ------------------------------\\ndef _parse_new_format_messages(data: Dict[str, Any], json_path: str) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\\n    \"\"\"\\n    Newer shape:\\n    {\"messages\":[\\n      {\"id\":\"...\",\"conversation_id\":\"...\",\"created_at\":1695000,\\n       \"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":\"...\"}, {\"type\":\"image_url\",\"url\":\"...\"}]}\\n    ]}\\n    Returns (curated_rows, media_rows)\\n    \"\"\"\\n    curated_rows, media_rows = [], []\\n    msgs = data.get(\"messages\")\\n    if not isinstance(msgs, list):\\n        return curated_rows, media_rows\\n\\n    unit_default = os.path.splitext(os.path.basename(json_path))[0]\\n\\n    for i, m in enumerate(msgs):\\n        unit_uid = m.get(\"conversation_id\") or unit_default\\n        segment_id = m.get(\"id\") or f\"seg_{i:06d}\"\\n        created_at = _to_utc_iso(m.get(\"created_at\") or m.get(\"create_time\") or m.get(\"timestamp\"))\\n        role = m.get(\"role\") or (m.get(\"author\") or {}).get(\"role\")\\n\\n        text_parts = []\\n        content = m.get(\"content\")\\n        if isinstance(content, list):\\n            for part in content:\\n                if not isinstance(part, dict): \\n                    continue\\n                t = (part.get(\"type\") or \"\").lower()\\n                if t == \"text\":\\n                    text = part.get(\"text\") or part.get(\"value\") or \"\"\\n                    if text:\\n                        text_parts.append(text)\\n                elif t in (\"image_url\", \"file\", \"attachment\"):\\n                    # Track media reference; if inside ZIP, we'll try to extract with same path\\n                    url_or_path = part.get(\"url\") or part.get(\"path\") or \"\"\\n                    ext = _infer_ext_from_mime_or_name(url_or_path)\\n                    kind = _kind_from_ext(ext)\\n                    media_id = _uuid5_like(unit_uid, f\"{segment_id}:{url_or_path or t}\")\\n                    media_rows.append({\\n                        \"media_id\": media_id,\\n                        \"unit_uid\": unit_uid,\\n                        \"segment_id\": segment_id,\\n                        \"kind\": kind,\\n                        \"filename\": os.path.basename(url_or_path) or f\"{media_id}{ext}\",\\n                        \"ext\": ext,\\n                        \"size_bytes\": None,\\n                        \"sha256\": None,\\n                        \"zip_internal_path\": url_or_path if url_or_path else None,\\n                        \"stored_path\": None,  # may be filled if embedded\\n                        \"source_ref\": json_path,\\n                        \"created_at\": created_at,\\n                    })\\n        elif isinstance(content, str):\\n            text_parts.append(content)\\n\\n        text_concat = \"\\n\".join([t for t in text_parts if t])\\n        curated_rows.append({\\n            \"unit_uid\": unit_uid,\\n            \"segment_id\": segment_id,\\n            \"created_at\": created_at,\\n            \"text\": text_concat,\\n            \"source_kind\": \"openai_chat\",\\n            \"source_ref\": json_path,\\n            \"content_hash\": _sha256_text(text_concat),\\n            \"role\": role or None,\\n        })\\n\\n    return curated_rows, media_rows\\n\\n\\ndef _parse_legacy_mapping(data: Dict[str, Any], json_path: str) -&gt; Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\\n    \"\"\"\\n    Legacy shape with a 'mapping' graph.\\n    Returns (curated_rows, media_rows)\\n    \"\"\"\\n    curated_rows, media_rows = [], []\\n    mapping = data.get(\"mapping\")\\n    if not isinstance(mapping, dict):\\n        return curated_rows, media_rows\\n\\n    unit_default = os.path.splitext(os.path.basename(json_path))[0]\\n    nodes = list(mapping.values())\\n\\n    # Preserve order by 'create_time' if present, else by index\\n    def _node_time(n):\\n        msg = n.get(\"message\", {})\\n        return msg.get(\"create_time\") or msg.get(\"created_at\") or 0\\n\\n    nodes.sort(key=_node_time)\\n\\n    for idx, node in enumerate(nodes):\\n        msg = node.get(\"message\") or {}\\n        # Some nodes are structural, skip if no content\\n        cont = msg.get(\"content\")\\n        if not cont:\\n            continue\\n\\n        unit_uid = msg.get(\"conversation_id\") or unit_default\\n        segment_id = msg.get(\"id\") or f\"seg_{idx:06d}\"\\n        created_at = _to_utc_iso(msg.get(\"create_time\") or msg.get(\"created_at\") or 0)\\n        role = (msg.get(\"author\") or {}).get(\"role\")\\n\\n        text_parts = []\\n        # legacy often stores content.parts: [\"text\", \"...\"]\\n        parts = cont.get(\"parts\") if isinstance(cont, dict) else None\\n        if isinstance(parts, list):\\n            for p in parts:\\n                if isinstance(p, str):\\n                    text_parts.append(p)\\n                elif isinstance(p, dict):\\n                    # very rare for legacy, but just in case\\n                    if (p.get(\"type\") or \"\").lower() == \"text\":\\n                        text_parts.append(p.get(\"text\") or \"\")\\n        else:\\n            # Sometimes 'content' is directly a string\\n            if isinstance(cont, str):\\n                text_parts.append(cont)\\n\\n        text_concat = \"\\n\".join([t for t in text_parts if t])\\n        curated_rows.append({\\n            \"unit_uid\": unit_uid,\\n            \"segment_id\": segment_id,\\n            \"created_at\": created_at,\\n            \"text\": text_concat,\\n            \"source_kind\": \"openai_chat\",\\n            \"source_ref\": json_path,\\n            \"content_hash\": _sha256_text(text_concat),\\n            \"role\": role or None,\\n        })\\n\\n    return curated_rows, media_rows\\n\\n\\n# ------------------------------\\n# Main entry\\n# ------------------------------\\ndef parse_export_to_curated_v2(zip_path: str, dataset_id: str = DEFAULT_DATASET_ID) -&gt; Dict[str, Any]:\\n    \"\"\"\\n    Parse a ChatGPT export ZIP:\\n      - Find JSON candidates\\n      - Normalize to curated v2 rows (tiny schema)\\n      - Extract embedded media to MEDIA_ROOT (dedup by sha256)\\n      - Save:\\n          * curated:&lt;dataset_id&gt;\\n          * media_manifest:&lt;dataset_id&gt;   (if any media found)\\n    Returns the curated artifact registry record (dict).\\n    \"\"\"\\n    curated_rows: List[Dict[str, Any]] = []\\n    media_rows: List[Dict[str, Any]] = []\\n\\n    with zipfile.ZipFile(zip_path) as z:\\n        names = [n for n in z.namelist() if _is_json_candidate(n)]\\n        for name in names:\\n            try:\\n                with z.open(name) as f:\\n                    raw = f.read()\\n                data = json.loads(raw.decode(\"utf-8\", errors=\"ignore\"))\\n            except Exception:\\n                continue\\n\\n            # Try new format first\\n            rows_new, media_new = _parse_new_format_messages(data, json_path=name)\\n            if rows_new:\\n                curated_rows.extend(rows_new)\\n                media_rows.extend(media_new)\\n                continue\\n\\n            # Fallback to legacy mapping\\n            rows_old, media_old = _parse_legacy_mapping(data, json_path=name)\\n            if rows_old:\\n                curated_rows.extend(rows_old)\\n                media_rows.extend(media_old)\\n\\n        # Attempt to materialize media that is actually embedded in the ZIP\\n        realized_media_rows: List[Dict[str, Any]] = []\\n        for mr in media_rows:\\n            ipath = mr.get(\"zip_internal_path\")\\n            if ipath:\\n                extracted = _extract_media_from_zip(z, ipath)\\n                if extracted:\\n                    stored_path, size_bytes, sha = extracted\\n                    mr = dict(mr)\\n                    mr[\"stored_path\"] = stored_path\\n                    mr[\"size_bytes\"] = size_bytes\\n                    mr[\"sha256\"] = sha\\n            realized_media_rows.append(mr)\\n\\n    # If no rows (unexpected), emit a safe demo row to keep pipeline shape intact\\n    if not curated_rows:\\n        curated_rows = [{\\n            \"unit_uid\": \"demo_unit\",\\n            \"segment_id\": \"seg_000001\",\\n            \"created_at\": _now_iso(),\\n            \"text\": \"Demo row (no messages found in export).\",\\n            \"source_kind\": \"openai_chat\",\\n            \"source_ref\": os.path.basename(zip_path),\\n            \"content_hash\": _sha256_text(\"Demo row (no messages found in export).\"),\\n            \"role\": None,\\n        }]\\n\\n    curated_df = pd.DataFrame(curated_rows)\\n    # Keep only minimal curated v2 columns; extra columns (role) are okay but optional\\n    curated_df = curated_df[[\\n        \"unit_uid\", \"segment_id\", \"created_at\", \"text\",\\n        \"source_kind\", \"source_ref\", \"content_hash\", \"role\"\\n    ]].copy()\\n\\n    curated_art = api.save_artifact(\\n        curated_df,\\n        stage=f\"curated:{dataset_id}\",\\n        parameters={\\n            \"dataset_id\": dataset_id,\\n            \"version\": datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\\n            \"schema\": {\\n                \"unit_uid\": \"str\",\\n                \"segment_id\": \"str\",\\n                \"created_at\": \"datetime_iso\",\\n                \"text\": \"str\",\\n                \"source_kind\": \"str\",\\n                \"source_ref\": \"str\",\\n                \"content_hash\": \"str\",\\n                \"role\": \"str?\",\\n            },\\n            \"notes\": \"Curated v2 (tiny schema). Text-only; media tracked separately in media_manifest.\"\\n        }\\n    )\\n\\n    # Save media manifest if any\\n    if realized_media_rows:\\n        media_df = pd.DataFrame(realized_media_rows)\\n        # Ensure consistent columns\\n        for col in [\"media_id\",\"unit_uid\",\"segment_id\",\"kind\",\"filename\",\"ext\",\"size_bytes\",\"sha256\",\"zip_internal_path\",\"stored_path\",\"source_ref\",\"created_at\"]:\\n            if col not in media_df.columns:\\n                media_df[col] = None\\n\\n        api.save_artifact(\\n            media_df,\\n            stage=f\"media_manifest:{dataset_id}\",\\n            parameters={\"dataset_id\": dataset_id},\\n            parents=[curated_art[\"id\"]],\\n        )\\n\\n    return curated_art</td>\n",
       "      <td>d1c6795285e96af12fa6cd5eb57a24fe4f228f8d2ac65d9bd80316a484bf0434</td>\n",
       "      <td>message</td>\n",
       "      <td>vectors_batch_f953107ed8</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>2456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.919371</td>\n",
       "      <td>16.446886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  conversation_id      batch_id        message_id sender             timestamp  \\\n",
       "0   conv_42a3e6cf  jsonl_import  msg_71c55fc69ed0   user  2025-09-25T21:04:05Z   \n",
       "1   conv_42a3e6cf  jsonl_import  msg_b765f8a4a016   user  2025-09-25T21:08:29Z   \n",
       "2   conv_42a3e6cf  jsonl_import  msg_dab4ac7c3b28   tool  2025-09-25T21:04:06Z   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             lets do a debugger of the parser--- in ipynb notebook   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     lets do a debugger of the parser--- give me cells i will copy them to a ipynb   \n",
       "2  Make sure to include fileciteturn0file0 in your response to cite this file. \\n\\n# src/proc/parse_openai.py\\nimport os\\nimport io\\nimport re\\nimport json\\nimport hashlib\\nimport zipfile\\nfrom datetime import datetime, timezone\\nfrom typing import Dict, Any, List, Tuple, Optional\\n\\nimport pandas as pd\\n\\n# Use your existing data layer\\nfrom _data_layer import api\\n\\n# ------------------------------\\n# Config via environment\\n# ------------------------------\\n# Where to store extracted media (outside Git; dedup by sha256)\\nMEDIA_ROOT = os.getenv(\"MEDIA_ROOT\", r\"D:\\data_hub\\media\\openai_chat\")\\n# Dataset id used for curated stage\\nDEFAULT_DATASET_ID = os.getenv(\"DATASET_ID\", \"openai_chat\")\\n\\n\\n# ------------------------------\\n# Utilities\\n# ------------------------------\\ndef _sha256_bytes(b: bytes) -> str:\\n    return hashlib.sha256(b).hexdigest()\\n\\n\\ndef _sha256_text(s: str) -> str:\\n    return hashlib.sha256((s or \"\").strip().encode(\"utf-8\")).hexdigest()\\n\\n\\ndef _now_iso() -> str:\\n    return datetime.now(timezone.utc).isoformat()\\n\\n\\ndef _to_utc_iso(ts) -> str:\\n    \"\"\"Best-effort normalization to UTC ISO 8601.\"\"\"\\n    try:\\n        if isinstance(ts, (int, float)):  # epoch seconds\\n            return datetime.fromtimestamp(ts, tz=timezone.utc).isoformat()\\n        if isinstance(ts, str):\\n            # Allow both Z and offset strings\\n            s = ts.replace(\"Z\", \"+00:00\")\\n            return datetime.fromisoformat(s).astimezone(timezone.utc).isoformat()\\n    except Exception:\\n        pass\\n    return _now_iso()\\n\\n\\ndef _safe_mkdir(p: str) -> None:\\n    os.makedirs(p, exist_ok=True)\\n\\n\\ndef _infer_ext_from_mime_or_name(name_or_mime: str) -> str:\\n    # very light heuristic; extend as needed\\n    s = (name_or_mime or \"\").lower()\\n    for ext in (\".png\", \".jpg\", \".jpeg\", \".gif\", \".webp\", \".bmp\", \".svg\", \".pdf\", \".txt\"):\\n        if s.endswith(ext) or ext in s:\\n            return ext\\n    if \"image/png\" in s: return \".png\"\\n    if \"image/jpeg\" in s: return \".jpg\"\\n    if \"image/gif\" in s: return \".gif\"\\n    if \"image/webp\" in s: return \".webp\"\\n    if \"application/pdf\" in s: return \".pdf\"\\n    return \".bin\"\\n\\n\\ndef _kind_from_ext(ext: str) -> str:\\n    ext = (ext or \"\").lower()\\n    if ext in [\".png\", \".jpg\", \".jpeg\", \".gif\", \".webp\", \".bmp\", \".svg\"]: return \"image\"\\n    if ext in [\".pdf\"]: return \"file\"\\n    return \"file\"\\n\\n\\ndef _uuid5_like(ns: str, name: str) -> str:\\n    # Deterministic id without importing uuid (keeps deps minimal)\\n    return hashlib.sha1((ns + \":\" + name).encode(\"utf-8\")).hexdigest()  # stable enough for ids\\n\\n\\n# ------------------------------\\n# JSON candidates inside ZIP\\n# ------------------------------\\nCANDIDATE_JSON_PATTERNS = [\\n    re.compile(r\"^conversations/.*\\.json$\", re.I),\\n    re.compile(r\"^chats?/.*\\.json$\", re.I),\\n    re.compile(r\"messages\\.json$\", re.I),\\n    re.compile(r\"conversations\\.json$\", re.I),\\n    re.compile(r\"chat_export\\.json$\", re.I),\\n]\\n\\n\\ndef _is_json_candidate(name: str) -> bool:\\n    n = name.replace(\"\\\\\", \"/\")\\n    return any(p.search(n) for p in CANDIDATE_JSON_PATTERNS)\\n\\n\\n# ------------------------------\\n# Media extraction (embedded in ZIP only)\\n# ------------------------------\\ndef _extract_media_from_zip(z: zipfile.ZipFile, internal_path: str) -> Optional[Tuple[str, int, str]]:\\n    \"\"\"\\n    If the ZIP contains media under internal_path, extract bytes, write to MEDIA_ROOT dedup store, return (stored_path, size_bytes, sha256).\\n    If not present, returns None (e.g., external URL-only reference).\\n    \"\"\"\\n    try:\\n        with z.open(internal_path) as fh:\\n            b = fh.read()\\n        sha = _sha256_bytes(b)\\n        ext = _infer_ext_from_mime_or_name(internal_path)\\n        obj_dir = os.path.join(MEDIA_ROOT, \"objects\")\\n        _safe_mkdir(obj_dir)\\n        stored_path = os.path.join(obj_dir, f\"{sha}{ext}\")\\n        if not os.path.exists(stored_path):\\n            with open(stored_path, \"wb\") as out:\\n                out.write(b)\\n        return stored_path, len(b), sha\\n    except KeyError:\\n        # Not embedded in ZIP\\n        return None\\n    except Exception:\\n        return None\\n\\n\\n# ------------------------------\\n# Parsers\\n# ------------------------------\\ndef _parse_new_format_messages(data: Dict[str, Any], json_path: str) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\\n    \"\"\"\\n    Newer shape:\\n    {\"messages\":[\\n      {\"id\":\"...\",\"conversation_id\":\"...\",\"created_at\":1695000,\\n       \"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":\"...\"}, {\"type\":\"image_url\",\"url\":\"...\"}]}\\n    ]}\\n    Returns (curated_rows, media_rows)\\n    \"\"\"\\n    curated_rows, media_rows = [], []\\n    msgs = data.get(\"messages\")\\n    if not isinstance(msgs, list):\\n        return curated_rows, media_rows\\n\\n    unit_default = os.path.splitext(os.path.basename(json_path))[0]\\n\\n    for i, m in enumerate(msgs):\\n        unit_uid = m.get(\"conversation_id\") or unit_default\\n        segment_id = m.get(\"id\") or f\"seg_{i:06d}\"\\n        created_at = _to_utc_iso(m.get(\"created_at\") or m.get(\"create_time\") or m.get(\"timestamp\"))\\n        role = m.get(\"role\") or (m.get(\"author\") or {}).get(\"role\")\\n\\n        text_parts = []\\n        content = m.get(\"content\")\\n        if isinstance(content, list):\\n            for part in content:\\n                if not isinstance(part, dict): \\n                    continue\\n                t = (part.get(\"type\") or \"\").lower()\\n                if t == \"text\":\\n                    text = part.get(\"text\") or part.get(\"value\") or \"\"\\n                    if text:\\n                        text_parts.append(text)\\n                elif t in (\"image_url\", \"file\", \"attachment\"):\\n                    # Track media reference; if inside ZIP, we'll try to extract with same path\\n                    url_or_path = part.get(\"url\") or part.get(\"path\") or \"\"\\n                    ext = _infer_ext_from_mime_or_name(url_or_path)\\n                    kind = _kind_from_ext(ext)\\n                    media_id = _uuid5_like(unit_uid, f\"{segment_id}:{url_or_path or t}\")\\n                    media_rows.append({\\n                        \"media_id\": media_id,\\n                        \"unit_uid\": unit_uid,\\n                        \"segment_id\": segment_id,\\n                        \"kind\": kind,\\n                        \"filename\": os.path.basename(url_or_path) or f\"{media_id}{ext}\",\\n                        \"ext\": ext,\\n                        \"size_bytes\": None,\\n                        \"sha256\": None,\\n                        \"zip_internal_path\": url_or_path if url_or_path else None,\\n                        \"stored_path\": None,  # may be filled if embedded\\n                        \"source_ref\": json_path,\\n                        \"created_at\": created_at,\\n                    })\\n        elif isinstance(content, str):\\n            text_parts.append(content)\\n\\n        text_concat = \"\\n\".join([t for t in text_parts if t])\\n        curated_rows.append({\\n            \"unit_uid\": unit_uid,\\n            \"segment_id\": segment_id,\\n            \"created_at\": created_at,\\n            \"text\": text_concat,\\n            \"source_kind\": \"openai_chat\",\\n            \"source_ref\": json_path,\\n            \"content_hash\": _sha256_text(text_concat),\\n            \"role\": role or None,\\n        })\\n\\n    return curated_rows, media_rows\\n\\n\\ndef _parse_legacy_mapping(data: Dict[str, Any], json_path: str) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\\n    \"\"\"\\n    Legacy shape with a 'mapping' graph.\\n    Returns (curated_rows, media_rows)\\n    \"\"\"\\n    curated_rows, media_rows = [], []\\n    mapping = data.get(\"mapping\")\\n    if not isinstance(mapping, dict):\\n        return curated_rows, media_rows\\n\\n    unit_default = os.path.splitext(os.path.basename(json_path))[0]\\n    nodes = list(mapping.values())\\n\\n    # Preserve order by 'create_time' if present, else by index\\n    def _node_time(n):\\n        msg = n.get(\"message\", {})\\n        return msg.get(\"create_time\") or msg.get(\"created_at\") or 0\\n\\n    nodes.sort(key=_node_time)\\n\\n    for idx, node in enumerate(nodes):\\n        msg = node.get(\"message\") or {}\\n        # Some nodes are structural, skip if no content\\n        cont = msg.get(\"content\")\\n        if not cont:\\n            continue\\n\\n        unit_uid = msg.get(\"conversation_id\") or unit_default\\n        segment_id = msg.get(\"id\") or f\"seg_{idx:06d}\"\\n        created_at = _to_utc_iso(msg.get(\"create_time\") or msg.get(\"created_at\") or 0)\\n        role = (msg.get(\"author\") or {}).get(\"role\")\\n\\n        text_parts = []\\n        # legacy often stores content.parts: [\"text\", \"...\"]\\n        parts = cont.get(\"parts\") if isinstance(cont, dict) else None\\n        if isinstance(parts, list):\\n            for p in parts:\\n                if isinstance(p, str):\\n                    text_parts.append(p)\\n                elif isinstance(p, dict):\\n                    # very rare for legacy, but just in case\\n                    if (p.get(\"type\") or \"\").lower() == \"text\":\\n                        text_parts.append(p.get(\"text\") or \"\")\\n        else:\\n            # Sometimes 'content' is directly a string\\n            if isinstance(cont, str):\\n                text_parts.append(cont)\\n\\n        text_concat = \"\\n\".join([t for t in text_parts if t])\\n        curated_rows.append({\\n            \"unit_uid\": unit_uid,\\n            \"segment_id\": segment_id,\\n            \"created_at\": created_at,\\n            \"text\": text_concat,\\n            \"source_kind\": \"openai_chat\",\\n            \"source_ref\": json_path,\\n            \"content_hash\": _sha256_text(text_concat),\\n            \"role\": role or None,\\n        })\\n\\n    return curated_rows, media_rows\\n\\n\\n# ------------------------------\\n# Main entry\\n# ------------------------------\\ndef parse_export_to_curated_v2(zip_path: str, dataset_id: str = DEFAULT_DATASET_ID) -> Dict[str, Any]:\\n    \"\"\"\\n    Parse a ChatGPT export ZIP:\\n      - Find JSON candidates\\n      - Normalize to curated v2 rows (tiny schema)\\n      - Extract embedded media to MEDIA_ROOT (dedup by sha256)\\n      - Save:\\n          * curated:<dataset_id>\\n          * media_manifest:<dataset_id>   (if any media found)\\n    Returns the curated artifact registry record (dict).\\n    \"\"\"\\n    curated_rows: List[Dict[str, Any]] = []\\n    media_rows: List[Dict[str, Any]] = []\\n\\n    with zipfile.ZipFile(zip_path) as z:\\n        names = [n for n in z.namelist() if _is_json_candidate(n)]\\n        for name in names:\\n            try:\\n                with z.open(name) as f:\\n                    raw = f.read()\\n                data = json.loads(raw.decode(\"utf-8\", errors=\"ignore\"))\\n            except Exception:\\n                continue\\n\\n            # Try new format first\\n            rows_new, media_new = _parse_new_format_messages(data, json_path=name)\\n            if rows_new:\\n                curated_rows.extend(rows_new)\\n                media_rows.extend(media_new)\\n                continue\\n\\n            # Fallback to legacy mapping\\n            rows_old, media_old = _parse_legacy_mapping(data, json_path=name)\\n            if rows_old:\\n                curated_rows.extend(rows_old)\\n                media_rows.extend(media_old)\\n\\n        # Attempt to materialize media that is actually embedded in the ZIP\\n        realized_media_rows: List[Dict[str, Any]] = []\\n        for mr in media_rows:\\n            ipath = mr.get(\"zip_internal_path\")\\n            if ipath:\\n                extracted = _extract_media_from_zip(z, ipath)\\n                if extracted:\\n                    stored_path, size_bytes, sha = extracted\\n                    mr = dict(mr)\\n                    mr[\"stored_path\"] = stored_path\\n                    mr[\"size_bytes\"] = size_bytes\\n                    mr[\"sha256\"] = sha\\n            realized_media_rows.append(mr)\\n\\n    # If no rows (unexpected), emit a safe demo row to keep pipeline shape intact\\n    if not curated_rows:\\n        curated_rows = [{\\n            \"unit_uid\": \"demo_unit\",\\n            \"segment_id\": \"seg_000001\",\\n            \"created_at\": _now_iso(),\\n            \"text\": \"Demo row (no messages found in export).\",\\n            \"source_kind\": \"openai_chat\",\\n            \"source_ref\": os.path.basename(zip_path),\\n            \"content_hash\": _sha256_text(\"Demo row (no messages found in export).\"),\\n            \"role\": None,\\n        }]\\n\\n    curated_df = pd.DataFrame(curated_rows)\\n    # Keep only minimal curated v2 columns; extra columns (role) are okay but optional\\n    curated_df = curated_df[[\\n        \"unit_uid\", \"segment_id\", \"created_at\", \"text\",\\n        \"source_kind\", \"source_ref\", \"content_hash\", \"role\"\\n    ]].copy()\\n\\n    curated_art = api.save_artifact(\\n        curated_df,\\n        stage=f\"curated:{dataset_id}\",\\n        parameters={\\n            \"dataset_id\": dataset_id,\\n            \"version\": datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\\n            \"schema\": {\\n                \"unit_uid\": \"str\",\\n                \"segment_id\": \"str\",\\n                \"created_at\": \"datetime_iso\",\\n                \"text\": \"str\",\\n                \"source_kind\": \"str\",\\n                \"source_ref\": \"str\",\\n                \"content_hash\": \"str\",\\n                \"role\": \"str?\",\\n            },\\n            \"notes\": \"Curated v2 (tiny schema). Text-only; media tracked separately in media_manifest.\"\\n        }\\n    )\\n\\n    # Save media manifest if any\\n    if realized_media_rows:\\n        media_df = pd.DataFrame(realized_media_rows)\\n        # Ensure consistent columns\\n        for col in [\"media_id\",\"unit_uid\",\"segment_id\",\"kind\",\"filename\",\"ext\",\"size_bytes\",\"sha256\",\"zip_internal_path\",\"stored_path\",\"source_ref\",\"created_at\"]:\\n            if col not in media_df.columns:\\n                media_df[col] = None\\n\\n        api.save_artifact(\\n            media_df,\\n            stage=f\"media_manifest:{dataset_id}\",\\n            parameters={\"dataset_id\": dataset_id},\\n            parents=[curated_art[\"id\"]],\\n        )\\n\\n    return curated_art   \n",
       "\n",
       "                                                       message_hash granularity           source_batch_id            source_model  cluster  strength    umap_x     umap_y  \n",
       "0  c01682962297ed7d0a76e1b2b993a00f2a18f4b9d7c8b17028a84a45ac961191     message  vectors_batch_f953107ed8  text-embedding-3-small     1089       1.0 -6.272159  -0.672664  \n",
       "1  3220c26f2ab21efe8a9023edc93cb2f7d083b9c80595ebf8757d8283ab7bf695     message  vectors_batch_f953107ed8  text-embedding-3-small     1089       1.0 -6.264522  -0.664268  \n",
       "2  d1c6795285e96af12fa6cd5eb57a24fe4f228f8d2ac65d9bd80316a484bf0434     message  vectors_batch_f953107ed8  text-embedding-3-small     2456       1.0 -3.919371  16.446886  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PARAMS:\n",
      "{\n",
      "  \"pipeline_order\": \"umap-first\",\n",
      "  \"mode\": \"new\",\n",
      "  \"update_strategy\": \"retrain\",\n",
      "  \"n_neighbors\": 15,\n",
      "  \"n_components\": 2,\n",
      "  \"umap_metric\": \"cosine\",\n",
      "  \"min_cluster_size\": 2,\n",
      "  \"min_samples\": 2,\n",
      "  \"cluster_selection_epsilon\": 0.0,\n",
      "  \"cluster_selection_method\": \"eom\",\n",
      "  \"hdb_metric\": \"euclidean\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from joblib import load\n",
    "import json\n",
    "\n",
    "# --- load (keep your same path) ---\n",
    "input_path = r\"C:/Projects/clasificador_mensajes_Copy - Copy/data/experiments/exp_mix_20250930T022031Z.joblib\"\n",
    "bundle = load(input_path)\n",
    "\n",
    "# --- pandas display: no truncation ---\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "assert isinstance(bundle, dict), f\"Expected dict bundle, got {type(bundle)}\"\n",
    "df = bundle.get(\"df\")\n",
    "params = bundle.get(\"params\")\n",
    "\n",
    "print(\"DF shape:\", None if df is None else df.shape)\n",
    "if df is not None:\n",
    "    print(\"\\nALL COLUMNS:\")\n",
    "    print(list(df.columns))\n",
    "\n",
    "    print(\"\\nDTYPES:\")\n",
    "    print(df.dtypes.to_string())\n",
    "\n",
    "    print(\"\\nHEAD(3):\")\n",
    "    display(df.head(3))\n",
    "\n",
    "# Pretty-print params (if present)\n",
    "if isinstance(params, dict):\n",
    "    print(\"\\nPARAMS:\")\n",
    "    print(json.dumps(params, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5efe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "- C:\\Projects\\clasificador_mensajes_Copy - Copy\\data\\experiments\\exp_mix_20250930T022031Z__for_embedding_atlas.csv\n",
      "- C:\\Projects\\clasificador_mensajes_Copy - Copy\\data\\experiments\\exp_mix_20250930T022031Z__for_embedding_atlas.parquet\n",
      "Note: used 2D projection fallback (umap_x, umap_y)\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
